{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-violence",
   "metadata": {},
   "source": [
    "# Measuring the input rate from Kafka to Parquet Factory\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "We need to know the rate of incoming records from Kafka and compare it to the current performance handling them in the Internal Data Pipeline in order to decide if we can handle the huge amount of stored archives to be reprocessed or not.\n",
    "    \n",
    "\n",
    "## About the task\n",
    "\n",
    "The first part of this task will consist in retrieve several information from the Kafka partitions in both topics and calculate the input rate from each topic and partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-muscle",
   "metadata": {},
   "source": [
    "## How to get the source data\n",
    "\n",
    "The relevant data that we need to retrieve from Kafka is the timestamps of the first and last messages from each partition and the offset of each message.\n",
    "\n",
    "These values are relevant because the timestamps difference will show us the amount of time, and the offset of both messages, the number of records sent to this specific topic.\n",
    "\n",
    "The following script can help to retrieve all the relevant data in a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-seminar",
   "metadata": {},
   "source": [
    "```bash\n",
    "TOPICS=\"ccx-prod-insights-operator-archive-rules-results ccx-prod-insights-operator-archive-features\"\n",
    "PARTITIONS=\"0 1\"\n",
    "OUTPUT=\"kafka_input.csv\"\n",
    "OPTIONS=\"\"\n",
    "BROKER=kafka:9092\n",
    "KAFKACAT_CMD=\"kafkacat -b $BROKER $OPTIONS -C \"\n",
    "\n",
    "echo \"msg,topic,partition,offset,timestamp\" > $OUTPUT\n",
    "\n",
    "for t in $TOPICS; do\n",
    "    for p in $PARTITIONS; do\n",
    "        $KAFKACAT_CMD -t $t -p $p -o beginning -c 1 -f \"beginning,%t,%p,%o,%T\\n\" >> $OUTPUT\n",
    "        $KAFKACAT_CMD -t $t -p $p -o -1 -c 1 -f \"end,%t,%p,%o,%T\\n\" >> $OUTPUT\n",
    "    done\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-price",
   "metadata": {},
   "source": [
    "Using `kafka_input.csv` as input file, we can calculate the number of messages per second received in each topic and partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = pd.read_csv(\"kafka_input.csv\")\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in set(rows[\"topic\"]):\n",
    "    accumulated_rate_topic = 0.0\n",
    "\n",
    "    for partition in set(rows.loc[(rows[\"topic\"] == topic)][\"partition\"]):\n",
    "        df = rows.loc[(rows[\"topic\"] == topic) & (rows[\"partition\"] == partition)]\n",
    "        beginning = df.loc[df[\"msg\"] == \"beginning\"]\n",
    "        end = df.loc[df[\"msg\"] == \"end\"]\n",
    "        initial_offset = int(beginning[\"offset\"])\n",
    "        initial_ts = int(beginning[\"timestamp\"]) / 1000.0\n",
    "        end_offset = int(end[\"offset\"])\n",
    "        end_ts = int(end[\"timestamp\"]) / 1000.0\n",
    "        num_records = end_offset - initial_offset\n",
    "        time_between = end_ts - initial_ts\n",
    "        partition_rate = num_records/time_between\n",
    "        accumulated_rate_topic += partition_rate\n",
    "        print(f\"\\t{topic} - {partition}: {partition_rate} records/s\")\n",
    "    \n",
    "    print(f\"\\nAccumulated rate for {topic}: {accumulated_rate_topic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-charleston",
   "metadata": {},
   "source": [
    "# Capacity of consuming records from Parquet Factory\n",
    "\n",
    "## How to get the source data\n",
    "\n",
    "The `parquet-factory` is run in an OpenShift cluster, where the pods have CPU and memory limits. For this reason, in order to get realistic data, I chose to take some measures for the last run `parquet-factory` instances.\n",
    "\n",
    "```bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-lightweight",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "PODNAMES=`oc get pods | awk '$1 ~ /parquet-factory/ { print $1 }'`\n",
    "\n",
    "if [[ $? != 0 ]]; then\n",
    "    echo \"Error retrieving the parquet-factory pods from cluster. Did you performed `oc login`?\"\n",
    "    exit 1\n",
    "else\n",
    "    echo \"Parquet Factory pods list retrieved\"\n",
    "fi\n",
    "OUTPUT=\"pods_timing.csv\"\n",
    "\n",
    "# Print header of the CSV\n",
    "echo \"pod_name;exit_status;start_time;end_time\" > ${OUTPUT}\n",
    "\n",
    "for pod in $PODNAMES; do\n",
    "    echo -n \"$pod;\" >> ${OUTPUT}\n",
    "    oc describe pod $pod | awk -v ORS=\";\" '/Started:|Finished:/ { $1=\"\"; print } /Exit\\sCode:/ { print $NF}' >> ${OUTPUT}\n",
    "    echo \"\" >> ${OUTPUT}\n",
    "done\n",
    "\n",
    "exit 0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_rows = pd.read_csv(\"pods_timing.csv\", sep=\";\", parse_dates=[\"start_time\", \"end_time\"])\n",
    "timing_rows[\"duration\"] = timing_rows[\"end_time\"].sub(timing_rows[\"start_time\"], axis=0).dt.seconds\n",
    "timing_rows[\"processing_time_per_record\"] = timing_rows[\"duration\"] / timing_rows[\"num_messages\"]\n",
    "timing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = timing_rows[\"duration\"] > 30.0\n",
    "timing_rows[relevant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = timing_rows[relevant][\"duration\"].mean()\n",
    "print(f'Average duration: {avg:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_processing = timing_rows[relevant][\"processing_time_per_record\"].mean()\n",
    "print(f\"Average processing time per record: {avg_processing:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-flower",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Parquet Factory

This program intends to aggregate a number of **rule-hits** reports from a Kafka topic and write several Parquet files.

Parquet Factory is able to upload the resulting files to an S3 server to long term storage, if configured.

If you need to explore further in the history of this repo, check the one in the internal GitHub instance under the `ccx` group.

## Documentation

For detailed documentation, see the [docs](docs/) directory:

- [Architecture](docs/architecture.md)
- [Configuration](docs/config.md)
- [CI/CD](docs/ci.md)
- [Deployment](docs/deployment.md)
- [Testing](docs/testing.md)
- [Metrics](docs/metrics.md)

# Incoming reports

This program consume the rule hits generated by the [rules service](https://github.com/RedHatInsights/data-pipeline#rules-service). That service writes its results in a Kafka topic, from where **Parquet Factory** consumes them in order to store in Parquet files.

To access to the produced messages, Kafkacat can be used:

```bash
$ kafkacat -b kafka.datahub.redhat.com:443 -X security.protocol=ssl -X ssl.ca.location=$DATAHUB_CA_CERT -C -t $TOPIC -c 1 | jq
```

Where `DATAHUB_CA_CERT` is the path to the DataHub CA certificate file, and `TOPIC` is any of the topics receiving produced messages in one of the environments:

* `ccx-prod-insights-operator-archive-rules-results` for production environment.
* `ccx-dev-insights-operator-archive-rules-results` for development environment.
* `ccx-qa-insights-operator-archive-rules-results` for QA environment.

# Local dev

There is a few steps that should be done for local dev:

0. Launch the command below to automatically generate mocks
for unit tests (if needed)
```
make gen_mocks
```

1. Start all necessary services
```
docker-compose up
```

2. Build and run parquet-factory
```
make build && ./parquet-factory
```

3. Push json messages to kafka
```
./produce.sh
```
